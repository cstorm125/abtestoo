{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing from Scratch: Frequentist Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequentist A/B testing is one of the most used and abused statistical methods in the world. This article starts with a simple problem of comparing two online ads campaigns (or teatments, user interfaces or slot machines). It outlines several useful statistical concepts and how we exploit them to solve our problem. At the end, it acknowledges some common pitfalls we face when doing a frequentist A/B test and proposes some possible solutions to a more robust A/B testing. Readers are encouraged to tinker with the widgets provided in order to explore the impacts of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Collection, Tuple\n",
    "\n",
    "#widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "#stats\n",
    "import scipy as sp\n",
    "import statsmodels as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with A Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical situation marketers (research physicians, UX researchers, or gamblers) find themselves in is that they have two variations of ads (treatments, user interfaces, or slot machines) and want to find out which one has the better performance in the long run.\n",
    "\n",
    "Practitioners know this as A/B testing and statisticians as **hypothesis testing**. Consider the following problem. We are running an online ads campaign `A` for a period of time, but now we think a new ads variation might work better so we run an experiemnt by dividing our audience in half: one sees the existing campaign `A` whereas the other sees a new campaign `B`. Our performance metric is conversion (sales) per click (ignore [ads attribution problem](https://support.google.com/analytics/answer/1662518) for now). After the experiment ran for two months, we obtain daily clicks and conversions of each campaign and determine which campaign has the better performance.\n",
    "\n",
    "We simulate the aforementioned problem with both campaigns getting randomly about a thousand clicks per day. The secrete we will pretend to not know is that hypothetical campaign `B` has slightly better conversion rate than `A` in the long run. With this synthetic data, we will explore some useful statistical concepts and exploit them for our frequentist A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bernoulli_campaign(p1: float, p2: float,\n",
    "                           lmh: Collection = [500, 1000, 1500],\n",
    "                           timesteps: int = 60,\n",
    "                           scaler: float = 300, seed: int = 1412) -> pd.DataFrame:\n",
    "    '''\n",
    "    :meth: generate fake impression-conversion campaign based on specified parameters\n",
    "    :param float p1: true conversion rate of group 1\n",
    "    :param float p2: true conversion rate of group 2\n",
    "    :param Collection lmh: low-, mid-, and high-points for the triangular distribution of clicks\n",
    "    :param int nb_days: number of timesteps the campaigns run for\n",
    "    :param float scaler: scaler for Gaussian noise\n",
    "    :param int seed: seed for Gaussian noise\n",
    "    :return: dataframe containing campaign results\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    ns = np.random.triangular(*lmh, size=timesteps * 2).astype(int)\n",
    "    np.random.seed(seed)\n",
    "    es = np.random.randn(timesteps * 2) / scaler\n",
    "\n",
    "    n1 = ns[:timesteps]\n",
    "    c1 = ((p1 + es[:timesteps]) * n1).astype(int)\n",
    "    n2 = ns[timesteps:]\n",
    "    c2 = ((p2 + es[timesteps:]) * n2).astype(int)\n",
    "    result = pd.DataFrame({'timesteps': range(timesteps), 'impression_a': n1, 'conv_a': c1, 'impression_b': n2, 'conv_b': c2})\n",
    "\n",
    "    result = result[['timesteps', 'impression_a', 'impression_b', 'conv_a', 'conv_b']]\n",
    "    result['cumu_impression_a'] = result.impression_a.cumsum()\n",
    "    result['cumu_impression_b'] = result.impression_b.cumsum()\n",
    "    result['cumu_conv_a'] = result.conv_a.cumsum()\n",
    "    result['cumu_conv_b'] = result.conv_b.cumsum()\n",
    "    result['cumu_rate_a'] = result.cumu_conv_a / result.cumu_impression_a\n",
    "    result['cumu_rate_b'] = result.cumu_conv_b / result.cumu_impression_b\n",
    "    return result\n",
    "\n",
    "conv_days = gen_bernoulli_campaign(p1 = 0.10,\n",
    "                          p2 = 0.105,\n",
    "                          timesteps = 60,\n",
    "                          scaler=300,\n",
    "                          seed = 1412) #god-mode \n",
    "conv_days.columns = [i.replace('impression','click') for i in conv_days.columns] #function uses impressions but we use clicks\n",
    "conv_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = conv_days[['timesteps','cumu_rate_a','cumu_rate_b']].melt(id_vars='timesteps')\n",
    "g = (ggplot(rates_df, aes(x='timesteps', y='value', color='variable')) + geom_line() + theme_minimal() +\n",
    "          xlab('Days of Experiment Run') + ylab('Cumulative Conversions / Cumulative Clicks'))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum after 2 months\n",
    "conv_df = pd.DataFrame({'campaign_id':['A','B'], 'clicks':[conv_days.click_a.sum(),conv_days.click_b.sum()],\n",
    "                        'conv_cnt':[conv_days.conv_a.sum(),conv_days.conv_b.sum()]})\n",
    "conv_df['conv_per'] =  conv_df['conv_cnt'] / conv_df['clicks']\n",
    "conv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables and Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a step back and think about the numbers we consider in our daily routines, whether it is conversion rate of an ads campaign, the relative risk of a patient group, or sales and revenues of a shop during a given period of time. From our perspective, they have one thing in common: **we do not know exactly how they come to be**. In fact, we would not need an A/B test if we do. For instance, if we know for certain that conversion rate of an ads campaign will be `0.05 + 0.001 * number of letters in the ads`, we can tell exactly which ads to run: the one with the highest number of letters in it.\n",
    "\n",
    "With our lack of knowledge, we do the next best thing and assume that our numbers are generated by some mathematical formula, calling them **random variables**. For instance, we might think of the probability of a click converting the same way as a coin-flip event, with the probability of converting as $p$ (say 0.1) and not converting as $1-p$ (thus 0.9). With this, we can simulate the event aka click conversion for as many times as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(n,p):\n",
    "    flips = np.random.choice([0,1], size=n, p=[1-p,p])\n",
    "    flips_df = pd.DataFrame(flips)\n",
    "    flips_df.columns = ['conv_flag']\n",
    "    g = (ggplot(flips_df,aes(x='factor(conv_flag)')) + geom_bar(aes(y = '(..count..)/sum(..count..)'))  + \n",
    "        theme_minimal() + xlab('Conversion Flag') + ylab('Percentage of Occurence') +\n",
    "        geom_hline(yintercept=p, colour='red') + ggtitle(f'Distribution after {n} Trials'))\n",
    "    g.draw()\n",
    "    print(f'Expectation: {p}\\nVariance: {p*(1-p)}')\n",
    "    print(f'Sample Mean: {np.mean(flips)}\\nSample Variance: {np.var(flips)}')\n",
    "\n",
    "interact(bernoulli, n=widgets.IntSlider(min=1,max=500,step=1,value=20),\n",
    "        p=widgets.FloatSlider(min=0.1,max=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability distribution** is represented with the values of a random variable we are interested in the X-axis, and the chance of them appearing after a number of trials in the Y-axis. The distribution above is called [Bernoulli Distribution](http://mathworld.wolfram.com/BernoulliDistribution.html), usually used to model hypothetical coin flips and online advertisements. [Other distributions](https://en.wikipedia.org/wiki/List_of_probability_distributions) are used in the same manner for other types of random variables. [Cloudera](https://www.cloudera.com/) provided a [quick review](https://medium.com/@srowen/common-probability-distributions-347e6b945ce4) on a few of them you might find useful.\n",
    "\n",
    "<img src='../images/distribution.png' alt='Common Probability Distributions; Cloudera'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two sets of indicators of a distribution that are especially relevant to our problem: one derived theoretically and another derived from data we observed. **Law of Large Numbers (LLN)** describes the relationship of between them.\n",
    "\n",
    "Theoretically, we can derive these values about any distribution:\n",
    "\n",
    "* **Expectation** of a random variable $X_i$ is its long-run average dervied from repetitively sampling $X_i$ from the same distribution. Each distribution requires its own way to obtain the expectation. For our example, it is the weighted average of outcomes $X_i$ ($X_i=1$ converted; $X_i=0$ not converted) and their respective probabilities ($p$ converted; $1-p$ not converted):\n",
    "\n",
    "\\begin{align}\n",
    "E[X_i] &= \\mu = \\sum_{i=1}^{k} p_i * X_i \\\\\n",
    "&= (1-p)*0 + p*1 \\\\\n",
    "&= p\n",
    "\\end{align}\n",
    "where $k$ is number of patterns of outcomes\n",
    "\n",
    "*  **Variance** of a random variable $X_i$ represents the expectation of how much $X_i$ deviates from its expectation, for our example formulated as:\n",
    "\n",
    "\\begin{align}\n",
    "Var(X_i) &= \\sigma^2 = E[(X_i-E(X_i))^2] \\\\\n",
    "&= E[X_i^2] - E[X_i]^2 \\\\\n",
    "&= \\{(1-p)*0^2 + p*1^2\\} - p^2 \\\\\n",
    "&= p(1-p)\n",
    "\\end{align}\n",
    "\n",
    "Empirically, we can also calculate their counterparts with the any amount of data we have on hand:\n",
    "\n",
    "* **Sample Mean** is simply an average of all $X_i$ we currently have in our sample of size $n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{X} &= \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "\\end{align}\n",
    "\n",
    "* **Sample Variance** is the variance based on deviation from sample mean; the $n-1$ is due to [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction#Source_of_bias) (See Appendix):\n",
    "\n",
    "\\begin{align}\n",
    "s^2 &= \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n",
    "\\end{align}\n",
    "\n",
    "LLN posits that when we have a large enough number of sample $n$, the sample mean will converge to expectation. This can be shown with a simple simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lln(n_max,p):\n",
    "    mean_flips = []\n",
    "    var_flips = []\n",
    "    ns = []\n",
    "    for n in range(1,n_max):\n",
    "        flips = np.random.choice([0,1], size=n, p=[1-p,p])\n",
    "        ns.append(n)\n",
    "        mean_flips.append(flips.mean())\n",
    "        var_flips.append(flips.var())\n",
    "    flips_df = pd.DataFrame({'n':ns,'mean_flips':mean_flips,'var_flips':var_flips}).melt(id_vars='n')\n",
    "    g = (ggplot(flips_df,aes(x='n',y='value',colour='variable')) + geom_line() +\n",
    "        facet_wrap('~variable', ncol=1, scales='free') + theme_minimal() +\n",
    "        ggtitle(f'Expectation={p:2f}; Variance={p*(1-p):2f}') + xlab('Number of Samples') +\n",
    "        ylab('Value'))\n",
    "    g.draw()\n",
    "\n",
    "interact(lln, n_max=widgets.IntSlider(min=2,max=10000,step=1,value=1000),\n",
    "        p=widgets.FloatSlider(min=0.1,max=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though LLN does not says that sample variance will also converge to variance as $n$ grows large enough, it is also the case. Mathematically, it can be derived as follows:\n",
    "\n",
    "\\begin{align}\n",
    "s^2 &= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X}^2) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\mu)^2 \\text{; as }n\\rightarrow\\infty\\text{ }\\bar{X}\\rightarrow\\mu\\\\\n",
    "&=\\frac{1}{n}(\\sum_{i=1}^{n}{X_i}^2 - 2\\mu\\sum_{i=1}^{n}X_i + n\\mu^2) \\\\\n",
    "&=\\frac{\\sum_{i=1}^{n}{X_i}^2}{n} - \\frac{2\\mu\\sum_{i=1}^{n}X_i}{n} + \\mu^2 \\\\\n",
    "&= \\frac{\\sum_{i=1}^{n}{X_i}^2}{n} - 2\\mu\\bar{X} + \\mu^2\\text{; as }\\frac{\\sum_{i=1}^{n}X_i}{n} = \\bar{X}\\\\\n",
    "&= \\frac{\\sum_{i=1}^{n}{X_i}^2}{n} - 2\\mu^2 + \\mu^2 = \\frac{\\sum_{i=1}^{n}{X_i}^2}{n} - \\mu^2 \\text{; as }n\\rightarrow\\infty\\text{ }\\bar{X}\\rightarrow\\mu\\\\\n",
    "&= E[{X_i}^2] - E[X_i]^2 = Var(X_i) = \\sigma^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming some probability distribution for our random variable also lets us exploit another extremely powerful statistical concept: **Central Limit Theorem (CLT)**. To see CLT in action, let us simplify our problem a bit and say we are only trying to find out if a hypothetical ads campaign `C` has a conversion rate of more than 10% or not, assuming data collected so far say that `C` has 1,000 clicks and 107 conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.DataFrame({'campaign_id':'C','clicks':1000,'conv_cnt':107,'conv_per':0.107},index=[0])\n",
    "c_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLT goes as follows: \n",
    "> If $X_i$ is an independent and identically distributed (i.i.d.) random variable with expectation $\\mu$ and variance $\\sigma^2$ and $\\bar{X_j}$ is the sample mean of $n$ samples of $X_i$ we drew as part of sample group $j$, then when $n$ is large enough, $\\bar{X_j}$ will follow a [normal distribution](http://mathworld.wolfram.com/NormalDistribution.html) with with expectation $\\mu$ and variance $\\frac{\\sigma^2}{n}$\n",
    "\n",
    "It is a mouthful to say and full of weird symbols, so let us break it down line by line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If $X_i$ is an independent and identically distributed (i.i.d.) random variable with expectation $\\mu$ and variance $\\sigma^2$** <br/>In our case, $X_i$ is if click $i$ is coverted ($X_i=1$) or not converted ($X_i=0$) with $\\mu$ as some probability that represents how likely a click will convert on average. *Independent* means that the probability of each click converting depends only on itself and not other clicks. *Identically distributed* means that the true probability of each click converting is more or less the same. We need to rely on domain knowledge to verify these assumptions; for example, in online advertisement, we would expect, at least for when working with a reputable ads network such as Criteo, that each click comes from indepdent users, as opposed to, say, a click farm where we would see a lot of clicks behaving the same way by design. Identical distribution is a little difficult to assume since we would think different demographics the ads are shown to will react differently so they might not have the same expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df = pd.DataFrame({'iid':[False]*100+[True]*100,\n",
    "                       'order': list(range(100)) + list(range(100)),\n",
    "                       'conv_flag':[1]*50+ [0]*50+ list(np.random.choice([0,1], size=100))})\n",
    "g = (ggplot(ind_df,aes(x='order',y='conv_flag',color='iid')) + geom_point() +\n",
    "     facet_wrap('~iid') + theme_minimal() + xlab('i-th Click') + ylab('Conversion') +\n",
    "     ggtitle('Both plots has conversion rate of 50% but only one is i.i.d.'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and $\\bar{X_j}$ is the sample mean of $n$ samples of $X_i$ we drew as part of sample group $j$, then**<br/>\n",
    "For campaign `C`, we can think of all the clicks we observed as one sample group, which exists in parallel with an infinite number of sample groups that we have not seen yet but can be drawn from the distribution by additional data collection. This way, we calculate the sample mean as total conversions divided by total number of clicks observed during the campaign.\n",
    "\n",
    "<img src='../images/sample_group.png' alt='Sample Group in Universe'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**when $n$ is large enough, $\\bar{X_j}$ will follow a [normal distribution](http://mathworld.wolfram.com/NormalDistribution.html) with with expectation $\\mu$ and variance $\\frac{\\sigma^2}{n}$**</br>\n",
    "Here's the kicker: regardless of what distribution each $X_i$ of sample group $j$ is drawn from, as long as you have enough number of sample $n$, the sample mean of that sample group $\\bar{X_j}$ will converge to a normal distribution. Try increase $n$ in the plot below and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt(n, dist):\n",
    "    n_total = n * 10000\n",
    "    if dist == 'discrete uniform':\n",
    "        r = np.random.uniform(size=n_total)\n",
    "    elif dist =='bernoulli':\n",
    "        r = np.random.choice([0,1],size=n_total,p=[0.9,0.1])\n",
    "    elif dist =='poisson':\n",
    "        r = np.random.poisson(size=n_total)\n",
    "    else:\n",
    "        raise ValueError('Choose distributions that are available')\n",
    "    #generate base distribution plot\n",
    "    r_df = pd.DataFrame({'r':r})\n",
    "    g1 = (ggplot(r_df, aes(x='r')) + geom_histogram(bins=30) + theme_minimal() +\n",
    "         xlab('Values') + ylab('Number of Samples') + \n",
    "         ggtitle(f'{dist} distribution where sample groups are drawn from'))\n",
    "    g1.draw()\n",
    "    \n",
    "    #generate sample mean distribution plot\n",
    "    normal_distribution = np.random.normal(loc=np.mean(r), scale=np.std(r) / np.sqrt(n), size=10000)\n",
    "    sm_df = pd.DataFrame({'sample_means':r.reshape(-1,n).mean(1),\n",
    "                          'normal_distribution': normal_distribution}).melt()\n",
    "    g2 = (ggplot(sm_df, aes(x='value',fill='variable')) + \n",
    "          geom_histogram(bins=30,position='nudge',alpha=0.5) + \n",
    "          theme_minimal() + xlab('Sample Means') + ylab('Number of Sample Means') + \n",
    "         ggtitle(f'Distribution of 10,000 sample means with size {n}')) \n",
    "    g2.draw()\n",
    "    \n",
    "\n",
    "dists = ['bernoulli','discrete uniform','poisson']\n",
    "interact(clt, n=widgets.IntSlider(min=1,max=100,value=30),\n",
    "         dist = widgets.Dropdown(\n",
    "                options=dists,\n",
    "                value='bernoulli')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation and variance of the sample mean distribution can be derived as follows:\n",
    "\n",
    "\\begin{align}\n",
    "E[\\bar{X_j}] &= E[\\frac{\\sum_{i=1}^{n} X_i}{n}] \\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\frac{1}{n} \\sum_{i=1}^{n} \\mu\\\\\n",
    "&= \\frac{n\\mu}{n} = \\mu \\\\\n",
    "Var(\\bar{X_j}) &= Var(\\frac{\\sum_{i=1}^{n} X_i}{n}) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{i=1}^{n} Var(X_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2\\\\\n",
    "&= \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that we know this specific normal distribution of sample means has expectation $\\mu$ and variance $\\frac{\\sigma^2}{n}$ is especially useful. Remember we want to find out whether campaign `C` **in general, not just in any sample group,** has better conversion rate than 10%. Below is that exact normal distribution based on information from our sample group (1,000 clicks) and the assumption that conversion rate is 10%:\n",
    "\n",
    "\\begin{align}\n",
    "E[\\bar{X_j}] &= \\mu = p\\\\\n",
    "&= 0.1 \\text{; by our assumption}\\\\\n",
    "Var(\\bar{X_j}) &= \\frac{\\sigma^2}{n} = \\frac{p*(1-p)}{n}\\\\\n",
    "&= \\frac{0.1 * (1-0.1)}{1000}\\\\\n",
    "&= 0.0009\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = c_df.clicks[0]\n",
    "x_bar = c_df.conv_per[0]\n",
    "p = 0.1\n",
    "mu = p; variance = p*(1-p)/n; sigma = (variance)**(0.5)\n",
    "# mu = 0; variance = 1; sigma = (variance)**(0.5)\n",
    "x = np.arange(0.05, 0.15, 1e-3)\n",
    "y = np.array([sp.stats.norm.pdf(i, loc=mu, scale=sigma) for i in x])\n",
    "\n",
    "sm_df = pd.DataFrame({'x': x, 'y': y, 'crit':[False if i>x_bar else True for i in x]})\n",
    "g = (ggplot(sm_df, aes(x='x', y='y')) + geom_area() +\n",
    "      theme_minimal() + xlab('Sample Means') + ylab('Probability Density Function') + \n",
    "      ggtitle('Sample mean distribution under our assumption')) \n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as we know the expectation (which we usually do as part of the assumption) and variance (which is more tricky) of the base distribution, we can use this normal distribution to model random variable from *any* distribution. That is, we can model *any* data as long as we can assume their expectation and variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think Like A ~~Detective~~ Frequentist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a frequentist perspective, we treat a problem like a criminal persecution. First, we assume innocence of the defendant often called **null hypothesis** (in our case that conversion rate is *less than or equal to* 10%). Then, we collect the evidence (all clicks and conversions from campaign `C`). After that, we review how *unlikely* it is that we have this evidence assuming the defendant is innocent (by looking at where our sample mean lands on the sample mean distribution). Most frequentist tests are simply saying:\n",
    "\n",
    ">If we assume that [conversion rate]() of [ads campaign C]() has the long-run [conversion rate]() of less than or equal to [10%](), our results with sample mean [0.107]() or more extreme ones are so unlikely that they happen only [23%]() of the time, calculated by the area of the distribution with higher value than our sample mean.\n",
    "\n",
    "Note that you can substitute the highlighted parts with any other numbers and statistics you are comparing; for instance, medical trials instead of ads campaigns and relative risks instead of converion rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (ggplot(sm_df, aes(x='x', y='y', group='crit')) + geom_area(aes(fill='crit')) +\n",
    "      theme_minimal() + xlab('Sample Means') + ylab('Probability Density Function') + \n",
    "      ggtitle('Sample mean distribution under our assumption') + \n",
    "      guides(fill=guide_legend(title=\"Conversion Rate < 0.1\"))) \n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether 23% is unlikely *beyond reasonable doubt* depends on how much we are willing to tolerate the false positive rate (the percentage of innocent people you are willing to execute). By convention, a lot of practioners set this to 1-5% depending on their problems; for instance, an experiment in physics may use 1% or less because physical phenomena is highly reproducible whereas social science may use 5% because the human behaviors are more variable. This is not to be confused with **false discovery rate** which is the probability of our positive predictions turning out to be wrong. The excellent book [Statistics Done Wrong](https://www.statisticsdonewrong.com/p-value.html) has given this topic an extensive coverage that you definitely should check out (Reinhart, 2015).\n",
    "\n",
    "This degree of acceptable unlikeliness is called **alpha** and the probability we observe is called **p-value**. We must set alpha as part of the assumption before looking at the data (the law must first state how bad an action is for a person to be executed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming A Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example of `C`, we are only interested when the conversion rate is *more than* 10% so we look only beyond the right-hand side of our sample mean (thus called **one-tailed tests**). If we were testing whether the conversion rate is *equal to* 10% or not we would be interested in both sides (thus called **two-tailed tests**). However, it is not straightforward since we have to know the equivalent position of our sample mean on the left-hand side of the distribution.\n",
    "\n",
    "One way to remedy this is to convert the sample mean distribution to a distribution that is symmetrical around zero and has a fixed variance so the value on one side is equivalent to minus that value of the other side. **Standard normal distribution** is the normal distribution with expectation $\\mu=0$ and variance $\\sigma^2=1$. We convert any normal distribution to a standard normal distribution by:\n",
    "\n",
    "1. Shift its expectation to zero. This can be done by substracting all values of a distribution by its expectation:\n",
    "\\begin{align}\n",
    "E[\\bar{X_j}-\\mu] &= E[\\bar{X_j}]-\\mu \\\\\n",
    "&= \\mu-\\mu \\\\\n",
    "&= 0 \\\\\n",
    "\\end{align}\n",
    "2. Scale its variance to 1. This can be done by dividing all values by square root of its variance called **standard deviation**:\n",
    "\\begin{align}\n",
    "Var(\\frac{\\bar{X_j}}{\\sqrt{\\sigma^2/n}}) &= \\frac{1}{\\sigma^2/n}Var(\\bar{X_j})\\\\\n",
    "&= \\frac{\\sigma^2/n}{\\sigma^2/n}\\\\\n",
    "&=1\n",
    "\\end{align}\n",
    "\n",
    "Try shifting and scaling the distribution below with different $m$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_normal(m,v):\n",
    "    n = c_df.clicks[0]\n",
    "    x_bar = c_df.conv_per[0]\n",
    "    p = 0.1\n",
    "    mu = p; variance = p*(1-p)/n; sigma = (variance)**(0.5)\n",
    "    x = np.arange(0.05, 0.15, 1e-3)\n",
    "    y = np.array([sp.stats.norm.pdf(i, loc=mu, scale=sigma) for i in x])\n",
    "    sm_df = pd.DataFrame({'x': x, 'y': y})\n",
    "    \n",
    "    #normalize process\n",
    "    sm_df['x'] = (sm_df.x - m) / np.sqrt(v)\n",
    "    sm_df['y'] = np.array([sp.stats.norm.pdf(i, loc=mu-m, scale=sigma/np.sqrt(v)) for i in sm_df.x])\n",
    "    print(f'Expectation of sample mean: {mu-m}; Variance of sample mean: {variance/v}')\n",
    "    g = (ggplot(sm_df, aes(x='x', y='y')) + geom_area() +\n",
    "          theme_minimal() + xlab('Sample Means') + ylab('Probability Density Function') + \n",
    "          ggtitle('Shifted Normal Distribution of Sample Mean')) \n",
    "    g.draw()\n",
    "    \n",
    "interact(shift_normal, \n",
    "         m=widgets.FloatSlider(min=-1e-1,max=1e-1,value=1e-1,step=1e-2),\n",
    "         v=widgets.FloatSlider(min=9e-5,max=9e-3,value=9e-5,step=1e-4, readout_format='.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By shifting and scaling, we can find out where `C`'s sample mean of 0.107 lands on the X-axis of a standard normal distribution:\n",
    "\\begin{align}\n",
    "\\bar{Z_j} &= \\frac{\\bar{X_j} - \\mu}{\\sigma / \\sqrt{n}} \\\\\n",
    "&= \\frac{0.107 - 0.1}{0.3 / \\sqrt{1000}} \\approx 0.7378648\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\bar{Z_j}$ and $-\\bar{Z_j}$, we can calculate the probability of falsely rejecting the null hypotheysis, or p-value, as the area in red, summing up to approximately 46%. This is most likely too high a false positive rate anyone is comfortable with (no one believes a pregnancy test that turns out positive for 46% of the people who are not pregnant), so we fail to reject the null hypothesis that conversion rate of `C` is equal to 10%. \n",
    "\n",
    "If someone asks a frequentist for an opinion, they would probably say that they cannot disprove `C` has conversion rate of 10% in the long run. If they were asked to choose an action, they would probably go with the course of action that assumes `C` has a conversion rate of 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = c_df.clicks[0]\n",
    "x_bar = c_df.conv_per[0]\n",
    "p = 0.1; mu = p; variance = p*(1-p)/n; sigma = (variance)**(0.5)\n",
    "x_bar_norm = (x_bar - mu) / sigma\n",
    "\n",
    "def standard_normal(x_bar_norm, legend_title):\n",
    "    x_bar_norm = abs(x_bar_norm)\n",
    "    x = np.arange(-3, 3, 1e-2)\n",
    "    y = np.array([sp.stats.norm.pdf(i, loc=0, scale=1) for i in x])\n",
    "    sm_df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "    #normalize process\n",
    "    sm_df['crit'] = sm_df.x.map(lambda x: False if ((x<-x_bar_norm)|(x>x_bar_norm)) else True)\n",
    "    g = (ggplot(sm_df, aes(x='x', y='y',group='crit')) + geom_area(aes(fill='crit')) +\n",
    "          theme_minimal() + xlab('Sample Means') + ylab('Probability Density Function') + \n",
    "          ggtitle('Standard Normal Distribution of Sample Mean') +\n",
    "          guides(fill=guide_legend(title=legend_title))) \n",
    "    g.draw()\n",
    "\n",
    "standard_normal(x_bar_norm, \"Conversion Rate = 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-test and More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CLT and standard normal distribution (sometimes called **Z-distribution**), we now have all the tools for one of the most popular and useful statistical hypothesis test, the **Z-test**. In fact we have already done it with the hypothetical campaign `C`. But let us go back to our original problem of comparing the long-run conversion rates of `A` and `B`. Let our null hypothesis be that they are equal to each other and alpha be 0.05 (we are comfortable with false positive rate of 5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to compare a random variable to a fixed value, but now we have two random variables from two ads campaign. We get around this by comparing **the difference of their sample mean** $\\bar{X_\\Delta} = \\bar{X_{A}} - \\bar{X_{B}}$ to 0. This way, our null hypothesis states that there is no difference between the long-run conversion rates of these campaigns. Through another useful statistical concept, we also know that the variance of $\\bar{X_\\Delta}$ is the sum of sample mean variances of $\\bar{X_\\text{A}}$ and $\\bar{X_\\text{B}}$ (Normal Sum Theorem; [Lemon, 2002](https://www.goodreads.com/book/show/3415974-an-introduction-to-stochastic-processes-in-physics)).\n",
    "\n",
    "Thus, we can calculate the **test statistic** or, specifically for Z-test, **Z-value** as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{Z_\\Delta} &= \\frac{\\bar{X_\\Delta}-\\mu}{\\sqrt{\\frac{\\sigma^2_\\text{A}}{n_\\text{A}} + \\frac{\\sigma^2_\\text{B}}{n_\\text{B}}}} \\\\\n",
    "&= \\frac{\\bar{X_\\Delta}-\\mu}{\\sqrt{\\sigma^2_\\text{pooled} * (\\frac{1}{n_\\text{A}} + \\frac{1}{n_\\text{B}})}} \n",
    "\\end{align}\n",
    "\n",
    "Since we are assuming that `A` and `B` has the same conversion rate, their variance is also assumed to be the same:\n",
    "\n",
    "$$\\sigma^2_{A} = \\sigma^2_{B} = \\sigma_\\text{pooled} = p * (1-p)$$\n",
    "\n",
    "where $p$ is the total conversions of both campaigns divided by their clicks (**pooled probability**).\n",
    "\n",
    "In light of the Z-value calculated from our data, we found that p-value of rejecting the null hypothesis that conversion rates of `A` and `B` are equal to each other is less than 3%, lower than our acceptable false positive rate of 5%, so we reject the null hypothesis that they perform equally well. The result of the test is **statistically significant**; that is, it is unlikely enough for us given the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_test(c1: int, c2: int,\n",
    "                    n1: int, n2: int,\n",
    "                    mode: str = 'one_sided') -> Tuple[float, float]:\n",
    "    '''\n",
    "    :meth: Z-test for difference in proportion\n",
    "    :param int c1: conversions for group 1\n",
    "    :param int c2: conversions for group 2\n",
    "    :param int n1: impressions for group 1\n",
    "    :param int n2: impressions for group 2\n",
    "    :param str mode: mode of test; `one_sided` or `two_sided`\n",
    "    :return: Z-score, p-value\n",
    "    '''\n",
    "    p = (c1 + c2) / (n1 + n2)\n",
    "    p1 = c1 / n1\n",
    "    p2 = c2 / n2\n",
    "    z = (p1 - p2) / np.sqrt(p * (1 - p) * (1 / n1 + 1 / n2))\n",
    "    if mode == 'two_sided':\n",
    "        p = 2 * (1 - sp.stats.norm.cdf(abs(z)))\n",
    "    elif mode == 'one_sided':\n",
    "        p = 1 - sp.stats.norm.cdf(abs(z))\n",
    "    else:\n",
    "        raise ValueError('Available modes are `one_sided` and `two_sided`')\n",
    "    return z, p\n",
    "\n",
    "z_value, p_value = proportion_test(c1=conv_df.conv_cnt[0], c2=conv_df.conv_cnt[1],\n",
    "                n1=conv_df.clicks[0], n2=conv_df.clicks[1], mode='two_sided')\n",
    "print(f'Z-value: {z_value}; p-value: {p_value}')\n",
    "\n",
    "standard_normal(z_value, \"No Difference in Conversion Rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rationale extends beyond comparing proportions such as conversion rates. For instance, we can also compare revenues of two different stores, assuming they are i.i.d. However in this case, we do not know the variance of the base distribution $\\sigma^2$, as it cannot be derived from our assumption (variance of Bernoulli distribution is $p*(1-p)$ but store revenues are not modelled after a coin flip). The test statistic then is created with sample variance $s^2$ based on our sample group and follows a slightly modified version of standard normal distribution (see [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test)). Your test statistics and sample mean distributions may change, but bottom line of frequentist A/B test is exploiting CLT and frequentist reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can calculate p-value from Z-value and vice versa. This gives us another canny way to look at the problem; that is, we can calculate the intervals where there is an arbitrary probability, say 95%, that sample mean of `A` or `B` will fall into. We call it **confidence interval**. You can see that despite us rejecting the null hypothesis that their difference is zero, the confidence intervals of both campaigns can still overlap. \n",
    "\n",
    "Try changing the number of conversion rate and clicks of each group as well as the alpha to see what changes in terms of p-value of Z-test and confidence intervals. You will see that the sample mean distribution gets \"wider\" as we have fewer samples in a group. Intuitively, this makes sense because the fewer clicks you have collected, the less information you have about true performance of an ads campaign and less confident you are about where it should be. So when designing an A/B test, you should plan to have similar number of sample between both sample groups in order to have similarly distributed sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_plot(c1: int, c2: int,\n",
    "                    n1: int, n2: int, alpha: float = 0.05,\n",
    "                    mode: str = 'one_sided') -> None:\n",
    "    '''\n",
    "    :meth: plot Z-test for difference in proportion and confidence intervals for each campaign\n",
    "    :param int c1: conversions for group 1\n",
    "    :param int c2: conversions for group 2\n",
    "    :param int n1: impressions for group 1\n",
    "    :param int n2: impressions for group 2\n",
    "    :param float alpha: alpha\n",
    "    :param str mode: mode of test; `one_sided` or `two_sided`\n",
    "    :return: None\n",
    "    '''\n",
    "    p = (c1 + c2) / (n1 + n2)\n",
    "    p1 = c1 / n1\n",
    "    p2 = c2 / n2\n",
    "    se1 = np.sqrt(p1 * (1 - p1) / n1)\n",
    "    se2 = np.sqrt(p2 * (1 - p2) / n2)\n",
    "    z = sp.stats.norm.ppf(1 - alpha / 2)\n",
    "    x1 = np.arange(p1 - 3 * se1, p1 + 3 * se1, 1e-4)\n",
    "    x2 = np.arange(p2 - 3 * se2, p2 + 3 * se2, 1e-4)\n",
    "    y1 = np.array([sp.stats.norm.pdf(i, loc=p1, scale=np.sqrt(p1 * (1 - p1) / n1)) for i in x1])\n",
    "    y2 = np.array([sp.stats.norm.pdf(i, loc=p2, scale=np.sqrt(p2 * (1 - p2) / n2)) for i in x2])\n",
    "    sm_df = pd.DataFrame({'campaign_id': ['Campaign A'] * len(x1) + ['Campaign B'] * len(x2),\n",
    "                          'x': np.concatenate([x1, x2]), 'y': np.concatenate([y1, y2])})\n",
    "\n",
    "    z_value, p_value = proportion_test(c1, c2, n1, n2, mode)\n",
    "    print(f'Z-value: {z_value}; p-value: {p_value}')\n",
    "\n",
    "    g = (ggplot(sm_df, aes(x='x', y='y', fill='campaign_id')) +\n",
    "         geom_area(alpha=0.5)\n",
    "         + theme_minimal() + xlab('Sample Mean Distribution of Each Campaign')\n",
    "         + ylab('Probability Density Function')\n",
    "         + geom_vline(xintercept=[p1 + se1 * z, p1 - se1 * z], colour='red')\n",
    "         + geom_vline(xintercept=[p2+se2*z, p2-se2*z], colour='blue')\n",
    "         + ggtitle(f'Confident Intervals at alpha={alpha}'))\n",
    "    g.draw()\n",
    "    \n",
    "interact(proportion_plot, \n",
    "         c1 = widgets.FloatSlider(min=0,max=70000,value=conv_df.conv_cnt[0]),\n",
    "         c2 = widgets.FloatSlider(min=0,max=70000,value=conv_df.conv_cnt[1]),\n",
    "         n1 = widgets.IntSlider(min=10,max=70000,value=conv_df.clicks[0]),\n",
    "         n2 = widgets.IntSlider(min=10,max=70000,value=conv_df.clicks[1]),\n",
    "         alpha = widgets.FloatSlider(min=0,max=1,value=0.05),\n",
    "         mode=fixed('one_sided'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Hypothesis Test Is Statistically Significant with Enough Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we generated the data, we know that conversion rate of campaign `A` (10%) is about 95% that of campaign `B` (10.5%). If we go with our gut feeling, most of us would say that they are practically the same; yet, our Z-test told us that they are different. The reason for this becomes apparent graphically when we decrease the number of clicks for both campaigns in the plot above. The Z-test stops becoming significant when both campaigns have about 50,000 clicks each, even though they still have exactly the same conversion rate. The culprit is our Z-value calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{Z_\\Delta} &= \\frac{\\bar{X_\\Delta}-\\mu}{\\sqrt{\\sigma^2_\\text{pooled} * (\\frac{1}{n_\\text{A}} + \\frac{1}{n_\\text{B}})}} \n",
    "\\end{align}\n",
    "\n",
    "Notice number of clicks $n_\\text{A}$ and $n_\\text{B}$ hiding in the denominator. Our test statistics $\\bar{Z_\\Delta}$ will go infinitely higher as long as we collect more clicks. If both campaigns `A` and `B` have one million clicks each, the difference of as small as 0.1% will be detected as statistically significant. Try adjusting the probabilities $p1$ and $p2$ in the plot below and see if the area of statistical significance expands or contracts as the difference between the two numbers changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance_plot(p1,p2):\n",
    "    n1s = pd.DataFrame({'n1':[10**i for i in range(1,7)],'k':0})\n",
    "    n2s = pd.DataFrame({'n2':[10**i for i in range(1,7)],'k':0})\n",
    "    ns = pd.merge(n1s,n2s,how='outer').drop('k',1)\n",
    "    ns['p_value'] = ns.apply(lambda row: proportion_test(p1*row['n1'], p2*row['n2'],row['n1'],row['n2'])[1], 1)\n",
    "    g = (ggplot(ns,aes(x='factor(n1)',y='factor(n2)',fill='p_value')) + geom_tile(aes(width=.95, height=.95)) +\n",
    "        geom_text(aes(label='round(p_value,3)'), size=10)+ theme_minimal() +\n",
    "        xlab('Number of Samples in A') + ylab('Number of Samples in B') +\n",
    "        guides(fill=guide_legend(title=\"p-value\")))\n",
    "    g.draw()\n",
    "\n",
    "interact(significance_plot, \n",
    "         p1 = widgets.FloatSlider(min=0,max=1,value=conv_df.conv_cnt[0] / conv_df.clicks[0],\n",
    "                                 step=1e-3,readout_format='.5f'),\n",
    "         p2 = widgets.FloatSlider(min=0,max=1,value=conv_df.conv_cnt[1] / conv_df.clicks[1],\n",
    "                                 step=1e-3,readout_format='.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More practically, look at cumulative conversion rates and z-values of `A` and `B` on a daily basis. Every day that we check the results based on cumulative clicks and conversions, we will come up with a different test statistic and p-value. Difference in conversion rates seem to stabilize after 20 days; however, notice that if you stop the test at day 25 or so, you would say it is NOT statistically significant, whereas if you wait a little longer, you will get the opposite result. The only thing that changes as time goes on is that we have more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (ggplot(rates_df, aes(x='timesteps', y='value', color='variable')) + geom_line() + theme_minimal() +\n",
    "          xlab('Days of Experiment Run') + ylab('Cumulative Conversions / Cumulative Clicks'))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "conv_days['cumu_z_value'] = conv_days.apply(lambda row: proportion_test(row['cumu_conv_a'], \n",
    "                                                                   row['cumu_conv_b'],row['cumu_click_a'], \n",
    "                                                                   row['cumu_click_b'], mode='two_sided')[0],1)\n",
    "conv_days['cumu_p_value'] = conv_days.apply(lambda row: proportion_test(row['cumu_conv_a'], \n",
    "                                                                   row['cumu_conv_b'],row['cumu_click_a'], \n",
    "                                                                   row['cumu_click_b'], mode='two_sided')[1],1)\n",
    "\n",
    "#plot\n",
    "g = (ggplot(conv_days, aes(x='timesteps',y='cumu_z_value',color='cumu_p_value')) + geom_line() + theme_minimal() +\n",
    "    xlab('Days of Campaign') + ylab('Z-value Calculated By Cumulative Data') +\n",
    "    geom_hline(yintercept=[sp.stats.norm.ppf(0.95),sp.stats.norm.ppf(0.05)], color=['red','green']) +\n",
    "    annotate(\"text\", label = \"Above this line A is better than B\", x = 20, y = 2, color = 'red') +\n",
    "    annotate(\"text\", label = \"Below this line B is better than A\", x = 20, y = -2, color = 'green'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Detectable Effect and Required Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We argue that this too-big-to-fail phenomena among sample groups is especially dangerous in the context of today's \"big data\" society. Gone are the days where statistical tests are done among two control groups of 100 people each using paper survey forms. Now companies are performning A/B testing between ad variations that could have tens of thousands or more samples (impressions or clicks), and potentially all of them will be \"statistically significant\". \n",
    "\n",
    "One way to remedy this is to do what frequentists do best: make more assumptions, more specifically **two** more. First, if we want to find out whether `B` has *better* conversion than `A`, we do not only make the null hypothesis (that `B` performs worse than or equally well as `A`) but **minimally by how much**. We can set **mininum detectable effect** as the smallest possible difference that would be worth investing the time and money in one campaign over the other; let say that from experience we think it is 1%. We then ask:\n",
    "\n",
    "> What is the mininum number of samples in a sample group (clicks in a campaign) should we have in order to reject the null hypothesis when the difference in sample means is [1%]()?\n",
    "\n",
    "The required number of samples in each group $n$ and $mn$ (where m is multiplier) in order for the test to reject a minimum detectable effect $\\text{MDE}$ at a certain alpha is:\n",
    "\\begin{align}\n",
    "Z_{\\alpha} &= \\frac{\\text{MDE}-\\mu}{\\sqrt{\\sigma^2 * (\\frac{1}{n} + \\frac{1}{mn})}} \\\\\n",
    "\\frac{(m+1)\\sigma^2}{mn} &= (\\frac{\\text{MDE}}{Z_{\\alpha}})^2 \\\\\n",
    "n &= \\frac{m+1}{m}(\\frac{Z_{\\alpha} \\sigma}{\\text{MDE}})^2 \\\\\n",
    "n &= 2(\\frac{Z_{\\alpha} \\sigma}{\\text{MDE}})^2; m=1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we make yet another crucial assumption about **the variance $\\sigma^2$ we expect**. Remember we used to estimate the variance by using the pooled probability of our sample groups, but here we have not even started the experiments. In a conventional A/B testing scenario, we are testing whether an experimental variation is better than the existing one, so one choice is **using sample variance of a campaign you are currently running**; for instance, if `A` is our current ads and we want to know if we should change to `B`, then we will use conversion rate of `A` from past time period to calculate the variance, say 10%.\n",
    "\n",
    "Let us go back in time before we even started our 2-month-long test between campaign `A` and `B`. Now we assume not only acceptable false positive rate alpha of 0.05 but also minimum detectable effect of 1% and expected variance of $\\sigma^2 = 0.1 * (1-0.1) = 0.09$, then we calculate that the minimum number of samples we should collect for each campaign. You can see that should we have done that we would have not been able to reject the null hypothesis, and stuck with campaign `A` going forward.\n",
    "\n",
    "The upside is that now we only have to run the test for about 5 days instead of 60 days assuming every day is the same for the campaigns (no peak traffic on weekends, for instance). The downside is that our null hypothesis gets much more specific with not only one but three assumptions:\n",
    "\n",
    "* Long-run conversion rate of `B` is no better than `A`'s\n",
    "* The difference that will matter to us is at least 1%\n",
    "* The expected variance conversion rates is $\\sigma^2 = 0.1 * (1-0.1) = 0.09$\n",
    "\n",
    "This fits many A/B testing scenarios since we might not want to change to a new variation even though it is better but not so much that we are willing to invest our time and money to change our current setup. Try adjusting $\\text{MDE}$ and $\\sigma$ in the plot below and see how the number of required samples change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_samples(mde: float, p: float, m: float = 1,\n",
    "                       alpha: float = 0.05, mode: str = 'one_sided') -> float:\n",
    "    '''\n",
    "    :meth: get number of required sample based on minimum detectable difference (in absolute terms)\n",
    "    :param float mde: minimum detectable difference\n",
    "    :param float p: pooled probability of both groups\n",
    "    :param float m: multiplier of number of samples; groups are n and nm\n",
    "    :param float alpha: alpha\n",
    "    :param str mode: mode of test; `one_sided` or `two_sided`\n",
    "    :return: estimated number of samples to get significance\n",
    "    '''\n",
    "    variance = p * (1 - p)\n",
    "    if mode == 'two_sided':\n",
    "        z = sp.stats.norm.ppf(1 - alpha / 2)\n",
    "    elif mode == 'one_sided':\n",
    "        z = sp.stats.norm.ppf(1 - alpha)\n",
    "    else:\n",
    "        raise ValueError('Available modes are `one_sided` and `two_sided`')\n",
    "    return (m + 1 / m) * variance * (z / mde)**2\n",
    "\n",
    "\n",
    "def plot_proportion_samples(mde, p, m=1, alpha=0.05, mode='one_sided'):\n",
    "    minimum_samples = proportion_samples(mde, p,m, alpha, mode)\n",
    "    g = (ggplot(conv_days, aes(x='cumu_click_a',y='cumu_z_value',color='cumu_p_value')) + geom_line() + \n",
    "        theme_minimal() +\n",
    "        xlab('Number of Samples per Campaign') + ylab('Z-value Calculated By Cumulative Data') +\n",
    "        geom_hline(yintercept=[sp.stats.norm.ppf(0.95),sp.stats.norm.ppf(0.05)], color=['red','green']) +\n",
    "        annotate(\"text\", label = \"Above this line A is better than B\", x = 30000, y = 2, color = 'red') +\n",
    "        annotate(\"text\", label = \"Below this line B is better than A\", x = 30000, y = -2, color = 'green') +\n",
    "        annotate(\"text\", label = f'Minimum required samples at MDE {mde}={int(minimum_samples)}', x = 30000, y = 0,) +\n",
    "        geom_vline(xintercept=minimum_samples))\n",
    "    g.draw()\n",
    "\n",
    "interact(plot_proportion_samples, mde = widgets.FloatSlider(min=0.001,max=0.01, value=0.01, step=1e-3,\n",
    "                                                          readout_format='.5f'),\n",
    "         p = widgets.FloatSlider(min=0.,max=1, value=0.1, step=1e-3, readout_format='.5f'),\n",
    "         m = widgets.FloatSlider(min=0.1,max=1, value=1, step=1e-1, readout_format='.5f'),\n",
    "         alpha = widgets.FloatSlider(min=0.01,max=0.1, value=0.05, step=1e-3, readout_format='.5f'),\n",
    "         mode = widgets.Dropdown(options=['one_sided','two_sided'], value='one_sided')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Will Get A Statistically Significant Result If You Try Enough Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept p-value represents is false positive rate of our test, that is, how unlikely it is to observe our sample groups given that they do not have different conversion rates in the long run. Let us re-simulate our campaigns `A` and `B` to have equal expectation of 10%. If we apply our current method, we can be comfortably sure we will not get statistical significance (unless we have an extremely large number of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_days = gen_bernoulli_campaign(p1 = 0.10,\n",
    "                          p2 = 0.10,\n",
    "                          timesteps = 60,\n",
    "                          scaler=100,\n",
    "                          seed = 1412) #god-mode \n",
    "conv_days.columns = [i.replace('impression','click') for i in conv_days.columns] #function uses impressions but we use clicks\n",
    "\n",
    "conv_days['cumu_z_value'] = conv_days.apply(lambda row: proportion_test(row['cumu_conv_a'], \n",
    "                                                                   row['cumu_conv_b'],row['cumu_click_a'], \n",
    "                                                                   row['cumu_click_b'], mode='two_sided')[0],1)\n",
    "conv_days['cumu_p_value'] = conv_days.apply(lambda row: proportion_test(row['cumu_conv_a'], \n",
    "                                                                   row['cumu_conv_b'],row['cumu_click_a'], \n",
    "                                                                   row['cumu_click_b'], mode='two_sided')[1],1)\n",
    "conv_days['z_value'] = conv_days.apply(lambda row: proportion_test(row['conv_a'], \n",
    "                                                                   row['conv_b'],row['click_a'], \n",
    "                                                                   row['click_b'], mode='two_sided')[0],1)\n",
    "conv_days['p_value'] = conv_days.apply(lambda row: proportion_test(row['conv_a'], \n",
    "                                                                   row['conv_b'],row['click_a'], \n",
    "                                                                   row['click_b'], mode='two_sided')[1],1)\n",
    "g = (ggplot(conv_days, aes(x='timesteps',y='cumu_z_value',color='cumu_p_value')) + geom_line() + theme_minimal() +\n",
    "    xlab('Days in Campaign') + ylab('Z-value Calculated By Cumulative Data') +\n",
    "    geom_hline(yintercept=[sp.stats.norm.ppf(0.975),sp.stats.norm.ppf(0.025)], color=['red','red']))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is instead of doing the test only once, we **do it every day using clicks and conversions of that day alone**. We will have 60 tests where 3 of them give statistically significant results that `A` and `B` have different conversion rates in the long run. The fact that we have exactly 5% of the tests turning positive despite knowing that none of them should is not a coincidence. The Z-value is calculated based on alpha of 5%, which means even if there is no difference at 5% of the time we perform this test with this specific set of assumptions we will still have a positive result ([Obligatory relevant xkcd strip](https://xkcd.com/882/); Munroe, n.d.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (ggplot(conv_days, aes(x='timesteps',y='z_value',color='p_value')) + geom_line() + theme_minimal() +\n",
    "    xlab('Each Day in Campaign') + ylab('Z-value Calculated By Daily Data') +\n",
    "    geom_hline(yintercept=[sp.stats.norm.ppf(0.975),sp.stats.norm.ppf(0.025)], color=['red','red']) +\n",
    "    ggtitle(f'We Have {(conv_days.p_value<0.05).sum()} False Positives Out of {conv_days.shape[0]} Days ({100*(conv_days.p_value<0.05).sum()/conv_days.shape[0]}%)'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many people will test online ads campaigns based on daily data, but many researchers perform repeated experiments and by necessity repeated A/B tests as shown above. If you have a reason to believe that sample groups from different experiments have the same distribution, you might consider grouping them together and perform one large test as usual. Otherwise, you can tinker the assumption of how much false positive you can tolerate. One such approach, among [others](https://en.wikipedia.org/wiki/Multiple_comparisons_problem), is the [Bonferroni correction](http://mathworld.wolfram.com/BonferroniCorrection.html). It scales your alpha down by the number of tests you perform to make sure that your false positive rate stays at most your original alpha. In our case, if we cale our alpha as$\\alpha_{\\text{new}}=\\frac{0.05}{60} \\approx 0.0008$, we will have the following statistically non-significant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (ggplot(conv_days, aes(x='timesteps',y='z_value',color='p_value')) + geom_line() + theme_minimal() +\n",
    "    xlab('Each Day in Campaign') + ylab('Z-value Calculated By Daily Data') +\n",
    "    geom_hline(yintercept=[sp.stats.norm.ppf(1-0.0008/2),sp.stats.norm.ppf(0.0008/2)], color=['red','red']) +\n",
    "    ggtitle(f'We Have {(conv_days.p_value<0.05).sum()} False Positives Out of {conv_days.shape[0]} Days ({100*(conv_days.p_value<0.05).sum()/conv_days.shape[0]}%)'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the best of our knowledge, the most reasonable and practical way to perform a frequentist A/B test is to know your assumptions, including but not limited to:\n",
    "\n",
    "* What distribution should your data be assumed to be drawn from? In many cases, we use Bernoulli distribution for proportions, Poisson distribution for counts and normal distribution for real numbers.\n",
    "* Are you comparing your sample group to a fixed value or another sample group?\n",
    "* Do you want to know if the expectation of the sample group is equal to, more than or less than its counterpart?\n",
    "* What is the minimum detectable effect and how many samples should you collect? What is a reasonable variance to assume in order to calculated required sample size?\n",
    "* What is the highest false positive rate $\\alpha$ that you can accept?\n",
    "\n",
    "With these assumptions cleared, you can most likely create a test statistics, then with frequentist reasoning, you can determine if the sample group you collected are unlikely enough that you would reject your null hypothesis because of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemons, D. S. (2002). An introduction to stochastic processes in physics. Baltimore: Johns Hopkins University Press.\n",
    "Normal Sum Theorem; p34\n",
    "* Munroe, Randall (n.d.). HOW TO Absurd Scientific Answers toCommon Real-world Problems. Retrieved from https://xkcd.com/882/\n",
    "* Reinhart, A. (2015, March 1). The p value and the base rate fallacy. Retrieved from https://www.statisticsdonewrong.com/p-value.html\n",
    "* [whuber](https://stats.stackexchange.com/users/919/whuber) (2017). Can a probability distribution value exceeding 1 be OK?. Retrieved from https://stats.stackexchange.com/q/4223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bessel's Correction for Sample Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random variables can be thought of as estimation of the real values such as sample variance is an estimation of variance from the \"true\" distribution. An estimator is said to be **biased** when its expectation is not equal to the true value (not to be confused with LLN where the estimator itself approaches the true value as number of samples grows).\n",
    "\n",
    "We can repeat the experiment we did for LLN with sample mean and true mean, but this time we compare how biased version ($\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$) and unbiased version ($\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$) of sample variance approach true variance as number of sample groups grow. Clearly, we can see that biased sample variance normally underestimates the true variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(x, dof=0):\n",
    "    n = x.shape[0]\n",
    "    mu = np.sum(x)/n\n",
    "    return np.sum((x - mu)**2) / (n-dof)\n",
    "\n",
    "n_total = 10000 #total number of stuff\n",
    "n_sample = 100 #number of samples per sample group\n",
    "sg_range = range(1,100) #number of sample groups to take average of sample variances from\n",
    "r = np.random.normal(loc=0,scale=1,size=n_total) #generate random variables based on Z distribution\n",
    "pop_var = var(r) #true variance of the population\n",
    "\n",
    "mean_s_bs = []\n",
    "mean_s_us = []\n",
    "\n",
    "for n_sg in sg_range:\n",
    "    s_bs = []\n",
    "    s_us =[]\n",
    "    for i in range(n_sg):\n",
    "        sg = np.random.choice(r,size=n_sample,replace=False)\n",
    "        s_bs.append(var(sg)) #biased sample variance\n",
    "        s_us.append(var(sg,1)) #unbiased sample variance\n",
    "    mean_s_bs.append(np.mean(s_bs))\n",
    "    mean_s_us.append(np.mean(s_us))\n",
    "s_df = pd.DataFrame({'nb_var':sg_range,'biased_var':mean_s_bs,\n",
    "                     'unbiased_var':mean_s_us}).melt(id_vars='nb_var')\n",
    "g = (ggplot(s_df,aes(x='nb_var',y='value',color='variable',group='variable')) + geom_line() + \n",
    "     geom_hline(yintercept=pop_var) + theme_minimal() +\n",
    "     xlab('Number of Sample Groups') + ylab('Sample Mean of Sample Variance in Each Group'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive exactly how much the bias is as follows:\n",
    "\n",
    "$$B[s_{biased}^2] = E[s_{biased}^2] - \\sigma^2 = E[s_{biased}^2 - \\sigma^2]$$\n",
    "\n",
    "where $B[s^2]$ is the bias of estimator (biased sample variance) $s_{biased}^2$ of variance $\\sigma^2$. Then we can calculate the bias as:\n",
    "\n",
    "\\begin{align}\n",
    "E[s_{biased}^2 - \\sigma^2] &= E[\\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X})^2 - \\frac{1}{n} \\sum_{i=1}^n(X_i - \\mu)^2] \\\\\n",
    "&= \\frac{1}{n}E[(\\sum_{i=1}^n X_i^2 -2\\bar{X}\\sum_{i=1}^n X_i + n\\bar{X^2}) - (\\sum_{i=1}^n X_i^2 -2\\mu\\sum_{i=1}^n X_i + n\\mu^2)] \\\\\n",
    "&= E[\\bar{X^2} - \\mu^2 - 2\\bar{X^2} + 2\\mu\\bar{X}] \\\\\n",
    "&= -E[\\bar{X^2} -2\\mu\\bar{X} +\\mu^2] \\\\\n",
    "&= -E[(\\bar{X} - \\mu)^2] \\\\\n",
    "&= -\\frac{\\sigma^2}{n} \\text{; variance of sample mean}\\\\\n",
    "E[s_{biased}^2] &= \\sigma^2 - \\frac{\\sigma^2}{n} \\\\ \n",
    "&= (1-\\frac{1}{n})\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "Therefore if we divide biased estimator $s_{biased}^2$ by $1-\\frac{1}{n}$, we will get an unbiased estimator of variance $s_{unbiased}^2$,\n",
    "\n",
    "\\begin{align}\n",
    "s_{unbiased}^2 &= \\frac{s_{biased}^2}{1-\\frac{1}{n}} \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X})^2}{1-\\frac{1}{n}}\\\\\n",
    "&= \\frac{1}{n-1} \\sum_{i=1}^n(X_i - \\bar{X})^2\n",
    "\\end{align}\n",
    "\n",
    "This is why the sample variance we usually use $s^2$ has $n-1$ instead of $n$. Also, this is not to be confused with the variance of sample means which is $\\frac{\\sigma^2}{n}$ when variance of the base distribution is known or assumed and $\\frac{s^2}{n}$ when it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass vs Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why the sample mean distribution has Y-axis that exceeds 1 even though it seemingly should represents probability of each value of sample mean. The short answer is that it does not represents probability but rather **probability density function**. The long answer is that there are two ways of representing probability distributions depending on whether they describe **discrete** or **continuous** data. See also this excellent [answer on Stack Exchange](https://stats.stackexchange.com/questions/4220/can-a-probability-distribution-value-exceeding-1-be-ok) (whuber, 2017). \n",
    "\n",
    "**Discrete probability distributions** contain values that are finite (for instance, $1, 2, 3, ...$) or countably infinite (for instance, $\\frac{1}{2^i}$ where $i=1, 2, 3, ...$). They include but not limited to distributions we have used to demonstrate CLT namely uniform, Bernoulli and Poisson distribution. In all these distributions, the Y-axis, now called **probability mass function**, represents the exact probability each value in the X-axis will take, such as the Bernouilli distribution we have shown before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flips = np.random.choice([0,1], size=n, p=[1-p,p])\n",
    "flips_df = pd.DataFrame(flips)\n",
    "flips_df.columns = ['conv_flag']\n",
    "g = (ggplot(flips_df,aes(x='factor(conv_flag)')) + geom_bar(aes(y = '(..count..)/sum(..count..)'))  + \n",
    "    theme_minimal() + xlab('Value') + ylab('Probability Mass Function') +\n",
    "    ggtitle(f'Bernoulli Distribution'))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous probability distribution** contains values that can take infinitely many, uncountable values (for instance, all real numbers between 0 and 1). Since there are infinitely many values, the probability of each individual value is essentially zero (what are the chance of winning the lottery that has infinite number of digits). Therefore, instead of the exact probability of each value (probability mass function), the Y-axis only represents the **probability density function**. This can be thought of as the total probability within an immeasurably small interval around the value. Take an example of a normal distribution with expectation $\\mu=0$ and variance $\\sigma^2=0.01$. The probability density function of the value 0 is described as:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi(0.01)}} e^{\\frac{-(x-0)^2}{2(0.01)}} \\text{; }\\mu=0;\\sigma^2=0.01 \\\\\n",
    "&\\approx 3.989 \\text{; when } x=0\n",
    "\\end{align}\n",
    "\n",
    "This of course does not mean that there is 398.9% chance that we will draw the value 0 but the density of the probability around the value. The actual probability of that interval around 0 is 3.989 times an immeasurably small number which will be between 0 and 1.\n",
    "\n",
    "Intuitively, we can think of these intervals as start from relatively large numbers such as 0.1 and gradually decreases to smaller numbers such as 0.005. As you can see from the plot below, the plot becomes more fine-grained and looks more \"normal\" as the intervals get smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_density(step,mu=0,sigma=0.1):\n",
    "    x = np.arange(-0.5, 0.5, step)\n",
    "    y = np.array([sp.stats.norm.pdf(i, loc=mu, scale=sigma) for i in x])\n",
    "\n",
    "    sm_df = pd.DataFrame({'x': x, 'y': y})\n",
    "    g = (ggplot(sm_df, aes(x='x', y='y')) + geom_bar(stat='identity') +\n",
    "          theme_minimal() + xlab('Value') + ylab('Probability Density Function') + \n",
    "          ggtitle(f'Normal Distribution with Expectation={mu} and Variance={sigma**2:2f}')) \n",
    "    g.draw()\n",
    "    \n",
    "interact(prob_density, step=widgets.FloatSlider(min=5e-3,max=1e-1,value=1e-1,step=1e-3,readout_format='.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Cov(X_i,Y_i) &= E[(X_i-E(X_i))(Y_i-E(Y_i))] \\\\\n",
    "&= E[X_i Y_i] - E[X_i]^2 \\\\\n",
    "&= E[X_i Y_i] - E[X_i]E[Y_i] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "Corr(X_i,Y_i) &= \\frac{E[(X_i-E(X_i))(Y_i-E(Y_i))]}{\\sigma_x \\sigma_y} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrections for Multiple Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bonferroni corrections - most conservative\n",
    "\n",
    "$$a_c = \\frac{a}{k}$$\n",
    "\n",
    "where k is number of experiments\n",
    "\n",
    "* Sidak corrections - assumes statistical independence among tests\n",
    "\n",
    "$$ a = 1 - (1-a_c)^k $$\n",
    "$$ a_c = 1 - (1-a)^{1/k} $$\n",
    "\n",
    "* Benjamini-Hochberg procedure \n",
    "\n",
    "    - Order m hypotheses according to lower p-value on a rank-p plane\n",
    "\n",
    "    - Draw a line $p < \\frac{a}{m} * rank$\n",
    "\n",
    "    - Find lowest ranks that are still above the line\n",
    "\n",
    "    - Limit the false discovery rate $FDR < \\frac{a}{m} * tp$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
